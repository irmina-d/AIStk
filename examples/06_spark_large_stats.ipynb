{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bb4e4d",
   "metadata": {},
   "source": [
    "# 06 â€” Spark Large Stats\n",
    "Cluster-scale statistics using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from aistk.backends.spark_backend import compute_stats_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('aistk-examples').getOrCreate()\n",
    "sdf = spark.read.parquet('lake/clean/2024')\n",
    "out = compute_stats_spark(sdf, level='mmsi')\n",
    "out.show(20, truncate=False)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
